from __future__ import absolute_import
from __future__ import division

import os

os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
import torch
from src import constant
from nltk.tokenize import word_tokenize

char2id = {'<UNK>': 0, ' ': 1, '‡§§': 2, '‡§≠': 3, '‡•Ä': 4, '‡§¨': 5, '‡§æ': 6, '‡§∞': 7, '‡§ø': 8, '‡§∂': 9, '‡§π': 10, '‡•Å': 11,
           '‡§à': 12, '‡§•': 13, '‡§ú': 14, '‡§∏': 15, '‡§ï': 16, '‡§ó': 17, '‡§≤': 18, '‡§™': 19, '‡§®': 20, '‡§á': 21, '‡§Æ': 22, '‡•Ç': 23,
           '‡•ç': 24, '‡§Ø': 25, '‡•ã': 26, '‡§Ç': 27, '‡§µ': 28, '‡•á': 29, '‡§è': 30, '‡§Ö': 31, '‡§¶': 32, '‡•à': 33, '.': 34, '‡§´': 35,
           '‡§º': 36, '‡§ß': 37, '‡§õ': 38, '/': 39, '‡§ö': 40, '‡§ü': 41, '‡§†': 42, '‡§ä': 43, '‡§£': 44, '‡§°': 45, '‡§î': 46, ',': 47,
           '‡§Å': 48, '‡§Ü': 49, '‡§ñ': 50, '‡§¢': 51, '‡§∑': 52, '‡§û': 53, '‡•§': 54, '‡§â': 55, '-': 56, '‡§ê': 57, '?': 58, '‡§ò': 59,
           '‡•å': 60, '‡§ì': 61, '2': 62, '4': 63, ':': 64, '‚Äù': 65, '‡•ú': 66, '!': 67, '‚Äò': 68, '‡§Ω': 69, '‡§ù': 70, '‡•û': 71,
           ';': 72, '‡•ù': 73, '\u200d': 74, '‡•õ': 75, '1': 76, '9': 77, '0': 78, '6': 79, '‡•É': 80, '‡§ã': 81, '‡§ë': 82,
           '(': 83, ')': 84, "'": 85, '‡•®': 86, '‡•ò': 87, '5': 88, '%': 89, '|': 90, '‡•â': 91, '‚Ä¶': 92, '‡•ß': 93, '‡•Ø': 94,
           '‡•™': 95, '‡•¶': 96, 's': 97, '‡•ä': 98, '‚Äô': 99, '‡••': 100, '‡•©': 101, '‚Äú': 102, '‡§É': 103, '8': 104, '3': 105,
           '\ufeff': 106, '+': 107, '‡•Æ': 108, '‡•´': 109, '‡•¨': 110, '*': 111, '7': 112, '‡•≠': 113, '[': 114, ']': 115,
           '\u200c': 116, '‡•ô': 117, '_': 118, '‡•ö': 119, '‡•†': 120, '‡•ê': 121, '‡•∞': 122, '‡•Ö': 123, '‚Äì': 124, '‡§±': 125,
           'i': 126, '‡§ô': 127, 'w': 128, 'a': 129, 'k': 130, 'h': 131, 'r': 132, 'c': 133, 'o': 134, 'm': 135, '‡•Ü': 136,
           'g': 137, '‡§é': 138, 'e': 139, 'z': 140, 'n': 141, '‚Äî': 142, '‚óè': 143, '¬©': 144, '&': 145, '‡§í': 146, '`': 147,
           '\uf0e8': 148, '>': 149, 't': 150, '‡§©': 151, '=': 152, '‡§≥': 153, '‡§ç': 154, '}': 155, '{': 156, '‡•°': 157,
           'd': 158, 'y': 159, 'p': 160, 'l': 161, 'u': 162, '#': 163, '‡•Ñ': 164, 'b': 165, '‚Ä¢': 166, '~': 167, 'f': 168,
           '\u200b': 169, '¬∑': 170, 'j': 171, '@': 172, '‡•ü': 173, '\\': 174, '¬∞': 175, '$': 176, '<': 177, '‡™≤': 178,
           '‡´ã': 179, '‡™•': 180, 'v': 181, 'q': 182, '\xad': 183, '¬£': 184, '‡•í': 185, '‡•î': 186}

# PATH = "{}/{}.pt".format(constant.params["model_dir"], constant.params["save_path"])
PATH = "saved_models/MODEL/model.pt"
model = torch.load(PATH, map_location=torch.device('cpu'))

if constant.USE_CUDA:
    model = model.cuda()

# for param in model.state_dict():
#     print(param)

model.eval()
vocab = model.vocab
# text_pipeline = vocab(word_tokenize())

label2id = {'HIN': 0, 'MAG': 1, 'ENG': 2}
id2label = {0: 'HIN', 1: 'MAG', 2: 'ENG'}


def get_tokens_id(text):
    tokens = word_tokenize(text)
    token_id = []
    for token in tokens:
        if token in vocab:
            token_id.append(vocab[token])
        else:
            token_id.append(0)
    return token_id


def get_char_id(text):
    """
    convert char to id
    :param text:
    :return:
    """
    char_id_list = []
    for char in text:
        if char in char2id:
            char_id_list.append(char2id[char])
        else:
            char_id_list.append(0)
    # padding for kernel
    while len(char_id_list) < 6:
        # append whitespace
        char_id_list.append(1)
    return char_id_list


def predict_label_id(model_lang, sentence, sentence_length, sent_word, sentence_word_lengths):
    with torch.no_grad():
        sent_word = torch.tensor([sent_word])
        sentence = torch.tensor([sentence])
        _, output = model_lang(X=sentence, X_lengths=sentence_length, supv_unsupv="un_supv", train_test="test",
                          x_word=sent_word, sentence_word_lengths=sentence_word_lengths)
        return output.argmax(1).item()


def lang_identify(model_lang, text):

    tokens_ids = get_tokens_id(text)
    sentence_token_length = len(tokens_ids)

    char_id_list = get_char_id(text)
    sentence_length = len(char_id_list)
    label_id = predict_label_id(model_lang, char_id_list, sentence_length, tokens_ids, sentence_token_length)
    lang_id = id2label[label_id]
    return lang_id


def main(model_lang, input_text_list):
    for text in input_text_list:
        word_tokens = word_tokenize(text)
        for word in word_tokens:
            lang = lang_identify(model_lang, word)
            print("-"*50)
            print(f"origin text: {word} \tdetected lang: {lang}")



if __name__ == "__main__":
    input_text = [
        "‡§ó‡§ú‡§¨ ‡§≠‡§à‡§Ø‡§æ‡•§ ‡§§‡•Ç ‡§≤‡•ã‡§ó ‡§Æ‡§ø‡§≤ ‡§ï‡•á ‡§ï‡§∞‡§æ ‡§¶‡•á‡§π‡•Å ‡§¨‡§ø‡§π‡§æ‡§∞ ‡§™‡•Å‡§≤‡§ø‡§∏ ‡§ï‡•á 14, ‡§§‡§æ‡§∞‡•Ä‡§ñ ‡§µ‡§æ‡§≤‡§æ Exam ‡§∞‡§¶‡•ç‡§¶ ‡§¨‡§∞‡•Ä ‡§®‡§æ‡§Æ ‡§π‡•ã ‡§§‡§à ‡§Ü‡§™ ‡§≤‡•ã‡§ó‡§® ‡§ï‡•á üôèüèªüôèüôèüôèüôè"
    ]
    main(model, input_text)